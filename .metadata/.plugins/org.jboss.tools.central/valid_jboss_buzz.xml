<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Composable software catalogs on Kubernetes: An easier way to update containerized applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Qu9uHZyp9gE/composable-software-catalogs-kubernetes-easier-way-update-containerized" /><author><name>David Festal</name></author><id>d42b5c6a-cc3d-45ed-9bd9-ac7ffb22b852</id><updated>2021-08-20T07:00:00Z</updated><published>2021-08-20T07:00:00Z</published><summary type="html">&lt;p&gt;Recently, I've been experimenting with how to build and use composable software catalogs on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Similar to &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; for &lt;a href="products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;, but adapted for a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; context, composable software catalogs let developers add tooling without building a new container image.&lt;/p&gt; &lt;p&gt;This article explains how composable software catalogs use existing container technologies to build on the Software Collections model, and how they can potentially make more options available to container users, simplify builds, and reduce container image sizes.&lt;/p&gt; &lt;h2&gt;Software Collections in a containerized world&lt;/h2&gt; &lt;p&gt;To understand the need for composable software catalogs, let's go back in time a bit.&lt;/p&gt; &lt;p&gt;Do you remember &lt;a href="https://www.softwarecollections.org/en/"&gt;Software Collections&lt;/a&gt;? The motto of this project, backed by Red Hat, was: &lt;em&gt;All versions of any software on your system. Together.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The promise was to build, install, and use multiple versions of software on the same system, without affecting system-wide installed packages. The key point was to create this multifold environment without affecting system-wide installed packages. In other words, it provided additional tooling without any change to the current state of the operating system as a whole. Software Collections worked well in its time, even winning a Top Innovator Award at DeveloperWeek 2014.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Red Hat Software Collections is available for Red Hat Enterprise Linux 7 and earlier supported releases. Starting with Red Hat Enterprise Linux 8, &lt;a href="https://access.redhat.com/support/policy/updates/rhscl"&gt;application streams replace Software Collections&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;A new landscape but the same need&lt;/h3&gt; &lt;p&gt;Things have changed since 2014. The container revolution popped up and brought features such as execution isolation, file system layering, and volume mounting. This solved quite a lot of problems. Thanks to containers, one could say that the old Software Collections became obsolete. But container orchestrators came along, as well (I'll stick to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; in this article). Deploying workloads as containers inside pods became standard. Finally, even workloads such as build pipelines or IDEs moved to the cloud and also ran inside containers.&lt;/p&gt; &lt;p&gt;But containers themselves have limitations. At some point, developers start experiencing the same type of need inside containers that Software Collections once tried to solve at the operating system level.&lt;/p&gt; &lt;h3&gt;Why revisit Software Collections?&lt;/h3&gt; &lt;p&gt;A &lt;em&gt;container&lt;/em&gt; is based on a single &lt;em&gt;container image&lt;/em&gt;, which is like a template for multiple identical containers. And a container image is optionally based on a single &lt;em&gt;container image&lt;/em&gt; &lt;em&gt;parent&lt;/em&gt;. To build a container image, you typically start from a basic operating system image. Then you add layers one by one, each on top of the previous one, to provide each additional tool or feature that you need in your container. Thus, each container is based on an image whose layers are overlays in a single inheritance tree. A container image is a snapshot of the current state of an operating system at a given point in time.&lt;/p&gt; &lt;p&gt;For container images, the old promise of Software Collections would be useful. In a container context, the goal of providing additional tooling &lt;em&gt;without any change to the current state of the operating system&lt;/em&gt; simply becomes &lt;em&gt;without having to build a new container image&lt;/em&gt;.&lt;/p&gt; &lt;h3&gt;A combinatorial explosion of components&lt;/h3&gt; &lt;p&gt;Let's take an example:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;I'd like to run a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; application—let's say the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/tree/master/getting-started"&gt;getting started example&lt;/a&gt;—directly from source code, in development mode. I will need at least a JDK version and a Maven version on top of the base operating system.&lt;/li&gt; &lt;li&gt;I'd also like to test the application with the widest available range of versions and flavors of the JDK, Maven, and the base operating system.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For each combination of the possible variants for those three components (JDK, Maven, and operating system), I would need to build a dedicated container image. And what if I also wanted to test with as many Gradle versions as possible? Not to mention including the native build use case, which requires GraalVM. Now imagine the combinatorial explosion that will occur if I decide to also include arbitrary versions of all my preferred tools.&lt;/p&gt; &lt;h3&gt;Inheritance versus composition&lt;/h3&gt; &lt;p&gt;The current manner of building containers limits us to a &lt;em&gt;single-inheritance model &lt;/em&gt;when what we need is &lt;em&gt;composition&lt;/em&gt;. Sometimes it would be great to be able to compose additional features or tools inside a container, without having to build a new container image. In fact, we just need to &lt;em&gt;compose container images&lt;/em&gt; at runtime. Obviously, allowing that in full generality seems tricky (if not impossible) to implement, at least given the current state of Kubernetes and containers. But what about a more limited case where we would only inject external self-contained tooling or read-only data into an existing container?&lt;/p&gt; &lt;h2&gt;Toward composable software catalogs on Kubernetes&lt;/h2&gt; &lt;p&gt;Injecting external self-contained tooling or read-only data into a container at runtime would obviously be particularly relevant if you think of things such as &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;, Maven, Gradle, even &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, NPM, Typescript, and the growing number of self-contained &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; utilities like Kubectl and Helm, as well as the Knative or Tekton CLI tools. None of them requires an "installation" process, strictly speaking. In order to start using them on most &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; variants of a given platform, you need only to download and extract them.&lt;/p&gt; &lt;h3&gt;Combining two container technologies&lt;/h3&gt; &lt;p&gt;Now let's introduce two container technologies that will allow us to implement this tool injection at runtime:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/"&gt;Container Storage Interface (CSI)&lt;/a&gt;, and more specifically &lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/"&gt;CSI Inline Ephemeral Volumes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://buildah.io/"&gt;Buildah&lt;/a&gt; containers&lt;/li&gt; &lt;/ul&gt;&lt;h4&gt;Container Storage Interface&lt;/h4&gt; &lt;p&gt;According to the &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/#why-csi"&gt;Kubernetes documentation&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;CSI opens many doors to implementing and integrating storage solutions into Kubernetes. On top of that, the &lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/"&gt;CSI Ephemeral Inline Volumes&lt;/a&gt; feature, still in beta, for now, allows you to specify a CSI volume, along with its parameters, directly in the pod spec, and only there. This is perfect to allow references, directly inside the pod, to the name of a tool to inject into pod containers.&lt;/p&gt; &lt;h4&gt;Buildah containers&lt;/h4&gt; &lt;p&gt;The &lt;code&gt;buildah&lt;/code&gt; tool is a well-known CLI tool that facilitates building Open Container Initiative (OCI) container images. Among many other features, it provides two that are very interesting for us:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Creating a container (from an image) that is not executing any command at the start, but can be manipulated, completed, and modified to possibly create a new image from it.&lt;/li&gt; &lt;li&gt;Mounting such a container to gain access to its underlying file system.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Buildah containers as CSI volumes&lt;/h2&gt; &lt;p&gt;The first attempt at combining CSI and &lt;code&gt;buildah&lt;/code&gt; started as a prototype example by the Kubernetes-CSI contributors, &lt;a href="https://github.com/kubernetes-csi/csi-driver-image-populator"&gt;csi-driver-image-populator&lt;/a&gt;. It was my main inspiration for the work shown in this article.&lt;/p&gt; &lt;p&gt;Providing a very lightweight and simple CSI driver, with the &lt;code&gt;image.csi.k8s.io&lt;/code&gt; identifier, &lt;code&gt;csi-driver-image-populator&lt;/code&gt; allows container images to be mounted as volumes. Deployed with a DaemonSet, the driver runs on each worker node of the Kubernetes cluster and waits for volume-mount requests. In the following example, a container image reference is specified in the pod as a parameter of the &lt;code&gt;image.csi.k8s.io&lt;/code&gt; CSI volume. Using &lt;code&gt;buildah&lt;/code&gt;, the corresponding CSI driver pulls the image, creates a container from it, and mounts its file system. The &lt;code&gt;buildah&lt;/code&gt; container filesystem is thus available to mount directly as a pod volume. Finally, the pod containers can reference this pod volume and use it:&lt;/p&gt; &lt;pre&gt; apiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: main image: main-container-image volumeMount: - name: &lt;strong&gt;composed-container-volume&lt;/strong&gt; mountPath: /somewhere-to-add-the-composed-container-filesystem volumes: - name: &lt;strong&gt;composed-container-volume&lt;/strong&gt; csi: driver: image.csi.k8s.io volumeAttributes: image: &lt;strong&gt;composed-container-image&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;Upon pod removal, the pod volume is unmounted by the driver, and the &lt;code&gt;buildah&lt;/code&gt; container is removed.&lt;/p&gt; &lt;h3&gt;Adapting the CSI driver for composable software catalogs&lt;/h3&gt; &lt;p&gt;Some aspects of the &lt;code&gt;csi-driver-image-populator&lt;/code&gt; prototype do not fit our use case for composable software catalogs:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;We don't need containers in the pod to have write access to composed image volumes. The whole idea in this article is to inject &lt;em&gt;read-only&lt;/em&gt; tools and data to the pod containers through the CSI inline volumes.&lt;/li&gt; &lt;li&gt;Sticking to the read-only use case allows us to use a single &lt;code&gt;buildah&lt;/code&gt; container for a given tool image, and share its mounted file system with all the pods that reference it. The number of &lt;code&gt;buildah&lt;/code&gt; containers then depends only on the number of images provided by the software catalog on the CSI driver side. This opens the door to additional performance optimizations.&lt;/li&gt; &lt;li&gt;For both performance and security reasons, we should avoid automatically pulling the container image mounted as a CSI inline volume. Let's pull images by an external component, outside the CSI driver. And let the CSI driver expose only images that were already pulled. Thus we limit the mounted images to a well-defined list of known images. In other words, we stick to a &lt;em&gt;managed software catalog&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Finally, for Kubernetes clusters that use an OCI-conformant container runtime (&lt;a href="https://cri-o.io/"&gt;cri-o&lt;/a&gt;, for example), we should be able to reuse images already pulled by the Kubernetes container runtime on the cluster node. This would take advantage of the image pulling capability of the Kubernetes distribution and comply with its configuration, instead of using a dedicated, distinct mechanism and configuration to pull a new image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To validate the idea described in this article, the changes just listed were implemented in a newly created CSI driver named &lt;a href="https://github.com/katalogos/csi-based-tool-provider"&gt;csi-based-tool-provider&lt;/a&gt;, starting from the &lt;code&gt;csi-driver-image-populator&lt;/code&gt; prototype to bootstrap the code.&lt;/p&gt; &lt;h3&gt;Providing dedicated tooling images&lt;/h3&gt; &lt;p&gt;In general, the new &lt;code&gt;csi-based-tool-provider&lt;/code&gt; driver is able to mount, as a pod read-only volume, any file system subpath of any container image. But still, it would be useful to define a typical structure for the container images that would populate such a software catalog. For "no-installation" software such as Java, which is simply delivered as an archive to extract, the most straightforward way to populate the catalog is to use "from scratch" images with the software directly extracted at the root of the filesystem. An example of a &lt;code&gt;Dockerfile&lt;/code&gt; for the &lt;a href="https://developers.redhat.com/products/openjdk/overview"&gt;OpenJDK&lt;/a&gt; 11 image would be:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/ubi as builder WORKDIR /build RUN curl -L https://github.com/AdoptOpenJDK/openjdk11-binaries/releases/download/jdk-11.0.9.1%2B1/OpenJDK11U-jdk_x64_linux_hotspot_11.0.9.1_1.tar.gz | tar xz FROM scratch WORKDIR / COPY --from=builder /build/jdk-11.0.9.1+1 . &lt;/pre&gt; &lt;p&gt;The same holds true for the Maven distribution required by our Quarkus example mentioned earlier. Next, we'll use the Quarkus example as a proof of concept (POC).&lt;/p&gt; &lt;h2&gt;Using composable software catalogs with Quarkus&lt;/h2&gt; &lt;p&gt;Now let's come back to our Quarkus example. I want to use only an interchangeable basic operating system for my container, without building any dedicated container image. And now I can manage additional tooling through CSI volume mounts on images from my new composable software catalog.&lt;/p&gt; &lt;p&gt;The full deployment looks like this:&lt;/p&gt; &lt;pre&gt; apiVersion: apps/v1 kind: Deployment metadata: name: csi-based-tool-provider-test spec: selector: matchLabels: app: csi-based-tool-provider-test replicas: 1 template: metadata: labels: app: csi-based-tool-provider-test spec: initContainers: - name: git-sync image: k8s.gcr.io/git-sync:v3.1.3 volumeMounts: - name: source mountPath: /tmp/git env: - name: HOME value: /tmp - name: GIT_SYNC_REPO value: https://github.com/quarkusio/quarkus-quickstarts.git - name: GIT_SYNC_DEST value: quarkus-quickstarts - name: GIT_SYNC_ONE_TIME value: "true" - name: GIT_SYNC_BRANCH value: 'main' containers: - name: main image: registry.access.redhat.com/ubi8/ubi args: - ./mvnw - compile - quarkus:dev - -Dquarkus.http.host=0.0.0.0 workingDir: /src/quarkus-quickstarts/getting-started ports: - containerPort: 8080 env: - name: HOME value: /tmp - name: JAVA_HOME value: /usr/lib/jvm/jdk-11 - name: M2_HOME value: /opt/apache-maven-3.6.3 volumeMounts: - name: java mountPath: /usr/lib/jvm/jdk-11 - name: maven mountPath: /opt/apache-maven-3.6.3 - name: source mountPath: /src volumes: - name: java csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-openjdk11u-jdk_x64_linux_hotspot_11.0.9.1_1:latest - name: maven csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-maven-3.6.3:latest - name: source emptyDir: {} &lt;/pre&gt; &lt;p&gt;To clone the example source code from GitHub, I reuse the &lt;a href="https://github.com/kubernetes/git-sync"&gt;git-sync&lt;/a&gt; utility inside an &lt;code&gt;initContainer&lt;/code&gt; of my Kubernetes &lt;code&gt;Deployment&lt;/code&gt;, but that's just for the sake of laziness and doesn't relate to the current work.&lt;/p&gt; &lt;h3&gt;Making tools available&lt;/h3&gt; &lt;p&gt;The first real interesting part of the implementation is:&lt;/p&gt; &lt;pre&gt; ... volumes: - name: java csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-openjdk11u-jdk_x64_linux_hotspot_11.0.9.1_1:latest - name: maven csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-maven-3.6.3:latest ... &lt;/pre&gt; &lt;p&gt;This configuration uses the new CSI driver to expose my two &lt;a href="https://github.com/katalogos/csi-based-tool-provider/tree/master/examples/catalog"&gt;tooling images&lt;/a&gt; as CSI read-only volumes.&lt;/p&gt; &lt;h3&gt;Mounting the tools&lt;/h3&gt; &lt;p&gt;The following configuration makes Java and Maven installations available for the main pod container to mount them at the needed place:&lt;/p&gt; &lt;pre&gt; ... containers: - name: main ... env: ... - name: JAVA_HOME value: /usr/lib/jvm/jdk-11 - name: M2_HOME value: /opt/apache-maven-3.6.3 volumeMounts: - name: java mountPath: /usr/lib/jvm/jdk-11 - name: maven mountPath: /opt/apache-maven-3.6.3 ... &lt;/pre&gt; &lt;p&gt;Note that the pod container owns the final path where the Java and Maven installations will be mounted. So the pod container can also set the related environment variables to the right paths.&lt;/p&gt; &lt;h3&gt;Using the mounted tools&lt;/h3&gt; &lt;p&gt;Finally, the container that will build and run the application source code in development mode can be based on a bare operating system image, and has nothing more to do than call the &lt;a href="https://quarkus.io/guides/getting-started#running-the-application"&gt;recommended startup command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; ... - name: main image: registry.access.redhat.com/ubi8/ubi args: - ./mvnw - compile - quarkus:dev - -Dquarkus.http.host=0.0.0.0 workingDir: /src/quarkus-quickstarts/getting-started ... &lt;/pre&gt; &lt;p&gt;The example will start on a &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image&lt;/a&gt;. But the great thing is that you can make changes, such as switching to an Ubuntu image, and the server will start and run the same way without any other change. And if you want to switch to another version of Maven, just change the reference to the corresponding container image in the &lt;code&gt;maven&lt;/code&gt; CSI volume.&lt;/p&gt; &lt;p&gt;If you scale up this deployment to ten pods, the same underlying Java and Maven installations will be used. No files will be duplicated on the disk, and no additional containers will be created on the cluster node. Only additional bind mounts will be issued on the cluster node. And the space savings will be the same, however many workloads use the Java and Maven tooling images on this node.&lt;/p&gt; &lt;h3&gt;What about performance?&lt;/h3&gt; &lt;p&gt;In the very first implementation, the new &lt;code&gt;csi-based-tool-provider&lt;/code&gt; driver ran &lt;a href="https://github.com/containers/buildah/blob/master/docs/buildah-manifest.md"&gt;buildah manifest&lt;/a&gt; commands to store the various metadata related to mounted images, along with the associated containers and volumes, inside an &lt;a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md#oci-image-manifest-specification"&gt;OCI manifest&lt;/a&gt;. Although this design was useful to get a POC working quickly, it required hard locks on the whole CSI mounting and unmounting process (&lt;code&gt;NodePublishVolume&lt;/code&gt; and &lt;code&gt;NodeUnpublishVolume&lt;/code&gt; CSI requests), in order to avoid concurrent modification of this global index and ensure consistency. Moreover, the &lt;code&gt;buildah&lt;/code&gt; container was initially created on the fly at mount time if necessary, and as soon as a given tool was not mounted by any pod container anymore, the corresponding &lt;code&gt;buildah&lt;/code&gt; container was removed by the CSI driver.&lt;/p&gt; &lt;p&gt;This design could lead to a mount delay of several seconds, especially when mounting an image for the first time. Instead of that design, the driver now uses an embeddable, high-performance, transactional key-value database called &lt;a href="https://github.com/dgraph-io/badger"&gt;BadgerDB&lt;/a&gt;. This choice allows much better performance and less contention caused by read-write locks. In addition, the list of container images exposed to the driver is now configured through a &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically"&gt;mounted ConfigMap&lt;/a&gt;. Images, as well as their related &lt;code&gt;buildah&lt;/code&gt; containers, are managed, created, and cleaned up in background tasks. These two simple changes have reduced the average volume mount delay to some fractions of a second, as shown by the graph of the related &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; metric in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/lJzq7mN.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/lJzq7mN.png?itok=k7FMYerB" width="556" height="265" alt="Composable software catalogs on Kubernetes: Average volume mount delay for a tool is between 15 and 20 ms." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The average volume mount delay for updated containers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Average volume mount delay for updated containers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;On a local &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;Minikube&lt;/a&gt; installation, for a simple pod containing one mounted CSI volume with the JDK image mentioned earlier, and one very simple container (doing nothing more than listing the content of the mounted volume and then sleeping), the average delay required to mount the JDK inside the Pod fluctuated between 15 and 20 milliseconds. In comparison with the overall pod startup duration (between 1 and 3 seconds), this is pretty insignificant.&lt;/p&gt; &lt;h3&gt;Testing the example&lt;/h3&gt; &lt;p&gt;The related code is available in the &lt;a href="https://github.com/katalogos/csi-based-tool-provider"&gt;csi-based-tool-provider&lt;/a&gt; GitHub repository, including instructions on how to test it using pre-built container images.&lt;/p&gt; &lt;h2&gt;Additional use cases for composable software catalogs&lt;/h2&gt; &lt;p&gt;Beyond the example used in this article, we can foresee concrete use cases where such tool injection would be useful. First, it reduces the combinatorial-explosion effect of having to manage, in a single container image, the versioning and lifecycle of both the underlying system and all the various system-independent tools. So it could reduce the overall size of image layers stored on Kubernetes cluster nodes.&lt;/p&gt; &lt;h3&gt;Red Hat OpenShift Web Terminal&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;Red Hat OpenShift Web Terminal&lt;/a&gt; is an example of a tool that could benefit from a software catalog. When opening a web terminal, the OpenShift console starts a pod with a container embedding all the typically required CLI tools. But if you need additional tools, you will have to replace this default container image with your own customized one, built by your own means. This build would not be necessary if we could provide all the CLI tools as volumes in a basic container. Composable software catalogs would also relieve the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) burden of having to rebuild the all-in-one container image each time one of the tools has to be updated. Going one step further, a catalog should allow using, in the web terminal, exactly the same version of the Kubernetes-related command-line tools (like &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;kubectl&lt;/code&gt;) as the version of the underlying OpenShift cluster.&lt;/p&gt; &lt;h3&gt;Tekton pipelines&lt;/h3&gt; &lt;p&gt;I also imagine how composable software catalogs could be used to inject off-the-shelf build tools into &lt;a href="https://github.com/tektoncd/pipeline/blob/master/docs/tasks.md#defining-steps"&gt;Tekton Task Steps&lt;/a&gt;. Here as well, there would be no more need to change and possibly rebuild Step container images each time you want to run your pipeline with different build tool variants or versions.&lt;/p&gt; &lt;h3&gt;Cloud IDEs&lt;/h3&gt; &lt;p&gt;Last but not least, composable software catalogs could benefit the various cloud-enabled IDEs, such as &lt;a href="https://www.eclipse.org/che/"&gt;Eclipse Che.&lt;/a&gt; The catalogs would make it really easy to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Efficiently switch the Java or Maven installations in a workspace&lt;/li&gt; &lt;li&gt;Share these installations among the various containers&lt;/li&gt; &lt;li&gt;Have several versions at the same time&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here as well, this new approach could greatly reduce the CI burden. We could stop building and maintaining a container image for each combination of underlying OS and tools. And composable software catalogs would finally unlock the combination of developer tools at runtime according to the developer's needs.&lt;/p&gt; &lt;h2&gt;What next?&lt;/h2&gt; &lt;p&gt;Although the proof of concept presented in this article is in an early alpha stage, we can already imagine some of the next steps to move it forward.&lt;/p&gt; &lt;h3&gt;Welcoming Katalogos&lt;/h3&gt; &lt;p&gt;A lot can be built on the foundation of the &lt;code&gt;csi-based-tool-provider&lt;/code&gt;. But as a first step, we should certainly set up a wider project dedicated to Kubernetes composable software catalogs. The CSI driver would be its first core component. So we've called this project Katalogos, from the ancient Greek word for a catalog: a register, especially one used for enrollment.&lt;/p&gt; &lt;h3&gt;Packaging the project as a complete solution&lt;/h3&gt; &lt;p&gt;Once the wider Katalogos project is bootstrapped, these next steps come to mind:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Add a Software Catalog Manager component to organize, pull, and manage images as software catalogs and make them available to the CSI driver on each cluster node.&lt;/li&gt; &lt;li&gt;Build an operator to install the CSI driver as well as configure the Software Catalog Manager.&lt;/li&gt; &lt;li&gt;Define a way to easily inject the required CSI volumes, as well as related environment variables, into pods according to annotations.&lt;/li&gt; &lt;li&gt;Provide related tooling and processes to easily build software catalogs that can feed the Software Catalog Manager.&lt;/li&gt; &lt;li&gt;Extend the mechanism to support the more complex case of software packages that are not inherently self-contained.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Getting feedback and building a community&lt;/h3&gt; &lt;p&gt;This article presented some ideas, with a minimal proof of concept, for a project that, I believe, could meet a real need in the current state of cloud-native development. The article is also a bid to get feedback, spark interest, and gather other use cases where the concept would fit. So, please comment, try the examples, open issues, fork the GitHub repository ... or simply star it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/20/composable-software-catalogs-kubernetes-easier-way-update-containerized" title="Composable software catalogs on Kubernetes: An easier way to update containerized applications"&gt;Composable software catalogs on Kubernetes: An easier way to update containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Qu9uHZyp9gE" height="1" width="1" alt=""/&gt;</summary><dc:creator>David Festal</dc:creator><dc:date>2021-08-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/20/composable-software-catalogs-kubernetes-easier-way-update-containerized</feedburner:origLink></entry><entry><title type="html">Season 3 and 60th Insights episode</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ansEqTkd-_k/" /><author><name /></author><id>https://quarkus.io/blog/60th-quarkus-insights/</id><updated>2021-08-20T00:00:00Z</updated><content type="html">After a summer break and little bit of COVID-19 delay, we will finally have the 60th(!) Quarkus Insights episode on Monday the 23rd August. For those who don’t know, Quarkus Insights is a (almost) weekly video/audio podcast where we host people from all parts of the Quarkiverse to sit down...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ansEqTkd-_k" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/60th-quarkus-insights/</feedburner:origLink></entry><entry><title>Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/J3giX7F6Bw4/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13" /><author><name>Josh Pinkney, Angel Misevski</name></author><id>fb68ba28-3337-4201-86cb-5d45789412d4</id><updated>2021-08-19T07:00:00Z</updated><published>2021-08-19T07:00:00Z</published><summary type="html">&lt;p&gt;The Web Terminal Operator in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; provides a web terminal with common cluster tooling pre-installed. The operator gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally.&lt;/p&gt; &lt;p&gt;This article is an overview of the new features introduced in Web Terminal Operator 1.3. These improvements include depending on the newly released DevWorkspace Operator, adding support for saving your home directory, and updating our tooling to be compatible with OpenShift 4.8.&lt;/p&gt; &lt;h2&gt;DevWorkspace Operator dependency&lt;/h2&gt; &lt;p&gt;Previously, the Web Terminal Operator relied on an embedded version of the &lt;a href="https://github.com/devfile/devworkspace-operator/"&gt;DevWorkspace controller&lt;/a&gt; to provide support for web terminals. That meant that in order to get the latest and greatest changes for the DevWorkspace controller, you had to wait three months for a new Web Terminal Operator release. As of Web Terminal 1.3, we have extracted that dependency, released the DevWorkspace Operator as its own separate operator, and now depend on that for providing support for web terminals. What this means for you is that the engine “under the hood” (the DevWorkspace Operator) will be updated every six weeks, instead of the standard three-month schedule for the Web Terminal Operator.&lt;/p&gt; &lt;h2&gt;Saving your home directory&lt;/h2&gt; &lt;p&gt;With Web Terminal 1.3, you will be able to mount your home directory and persist changes to your web terminal over multiple restarts. This feature isn’t enabled by default and it requires some additional configuration. This additional configuration depends on whether or not you already have a web terminal.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Only a web terminal that has a volume and volume mount for &lt;code&gt;/home/user&lt;/code&gt; will persist the home directory. Details follow.&lt;/p&gt; &lt;h3&gt;If you already have a web terminal&lt;/h3&gt; &lt;p&gt;In order to persist the home directory over multiple restarts, you will need to update the custom resource to mount a volume into the web terminal tooling container:&lt;/p&gt; &lt;p&gt;First, get your web terminal custom resource in your namespace and open it up for editing:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get dw -n ${your_namespace} oc edit dw ${your web terminal name} -n ${your_namespace}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that you have your custom resource open for editing, modify the DevWorkspace custom resource components section to add a mount for the storage. This can be done by first defining a volume in &lt;code&gt;spec.template.components&lt;/code&gt; and then mounting that volume in the web terminal tooling container. The highlighted portion of the YAML file in Figure 1 shows what you need to add in order to get the mount.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/terminal_mount.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/terminal_mount.png?itok=n82YvfnY" width="580" height="612" alt="The configuration of your DevWorkspace must have a section mounting the /home/user directory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Configuration file with a section mounting the /home/user directory. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once your custom resource has been edited and saved, you can start a new web terminal and create changes under &lt;code&gt;/home/user&lt;/code&gt;. Your changes will persist over multiple restarts.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are editing the custom resource inside a web terminal, the web terminal will automatically apply the changes and then close the active web terminal. To reopen the web terminal with the changes applied, click the &lt;strong&gt;Restart Terminal&lt;/strong&gt; button.&lt;/p&gt; &lt;h3&gt;If you don’t already have a web terminal&lt;/h3&gt; &lt;p&gt;In order to persist the home directory if you don’t already have a web terminal, you can edit your configuration file and add the following fields. In the YAML shown, replace &lt;code&gt;${your namespace}&lt;/code&gt; with the namespace where you want to create the web terminal.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are an admin, you must create your web terminal in &lt;code&gt;openshift-terminal&lt;/code&gt;, otherwise you will not be able to connect to the web terminal through the OpenShift web console. If the &lt;code&gt;openshift-terminal&lt;/code&gt; namespace does not currently exist, you must start your first web terminal through the UI so that the namespace is provisioned.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: DevWorkspace apiVersion: workspace.devfile.io/v1alpha2 metadata: name: web-terminal namespace: ${your namespace} annotations: controller.devfile.io/restricted-access: "true" labels: console.openshift.io/terminal: "true" spec: started: true routingClass: 'web-terminal' template: components: - name: web-terminal-exec plugin: kubernetes: name: web-terminal-exec namespace: openshift-operators - name: web-terminal-tooling plugin: kubernetes: name: web-terminal-tooling namespace: openshift-operators components: - name: web-terminal-tooling container: volumeMounts: - name: home-storage path: "/home/user" - name: home-storage volume: {}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once your web terminal custom resource has been created, you can start a web terminal and all your changes under &lt;code&gt;/home/user&lt;/code&gt; will be persisted by default.&lt;/p&gt; &lt;h2&gt;Tooling update&lt;/h2&gt; &lt;p&gt;We have updated the default binaries in Web Terminal Operator 1.3 to include the latest versions of the built-in command-line tools, as shown in Table 1.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="388"&gt;&lt;caption&gt;Table 1: Command-line tools in Web Terminal Operator 1.3.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Binary&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Old version&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;New version&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;oc&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4.7.0&lt;/td&gt; &lt;td&gt;4.8.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v1.20.1&lt;/td&gt; &lt;td&gt;v0.21.0-beta.1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;odo&lt;/code&gt;&lt;/td&gt; &lt;td&gt;2.0.4&lt;/td&gt; &lt;td&gt;2.2.3&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;knative&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.19.1&lt;/td&gt; &lt;td&gt;0.21.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;tekton&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.15.0&lt;/td&gt; &lt;td&gt;0.17.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubectx&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v0.9.3&lt;/td&gt; &lt;td&gt;v0.9.4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubens&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v0.9.3&lt;/td&gt; &lt;td&gt;v0.9.4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;rhoas&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.24.1&lt;/td&gt; &lt;td&gt;0.25.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;submariner&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;0.9.1 (First release)&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h2&gt;Try out these features&lt;/h2&gt; &lt;p&gt;In Web Terminal 1.3 we have changed the default channel from alpha to fast. This means that you’ll have to go through some extra steps to try out these new features:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;If your cluster already has the Web Terminal Operator installed, uninstall it by following the &lt;a href="https://docs.openshift.com/container-platform/4.7/web_console/odc-about-web-terminal.html#deleting-the-web-terminal-components-and-custom-resources"&gt;uninstall instructions&lt;/a&gt; and re-install the Web Terminal Operator using the fast channel.&lt;/li&gt; &lt;li&gt;If your cluster does not have the Web Terminal Operator installed, install it using the fast channel.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;For a peek into how the Web Terminal Operator works under the hood, please see &lt;a href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;A deeper look at the Web Terminal Operator&lt;/a&gt; by Angel Misevski. You can also check out the initial release article by Joshua Wood: &lt;a href="https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/"&gt;Command-line cluster management with Red Hat OpenShift’s new web terminal&lt;/a&gt;. For a look at our previous release blog, read &lt;a href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2"&gt;What’s new in Red Hat OpenShift's Web Terminal Operator 1.2&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/19/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13" title="Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3"&gt;Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/J3giX7F6Bw4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Josh Pinkney, Angel Misevski</dc:creator><dc:date>2021-08-19T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/19/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13</feedburner:origLink></entry><entry><title type="html">Quarkus 2.1.3.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ygixatckB04/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-1-3-final-released/</id><updated>2021-08-19T00:00:00Z</updated><content type="html">We just released Quarkus 2.1.3.Final, our third maintenance release on top of 2.1. It is a safe upgrade for anyone already using 2.1. If you are not using 2.1 already, please refer to the 2.1 migration guide. Full changelog You can get the full changelog of 2.1.3.Final on GitHub. Come...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ygixatckB04" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-1-3-final-released/</feedburner:origLink></entry><entry><title>Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WDsW_vxS2Eg/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology" /><author><name>Olivier Rivat</name></author><id>555ef949-fa8a-4a66-a0c8-69be98359164</id><updated>2021-08-18T07:00:00Z</updated><published>2021-08-18T07:00:00Z</published><summary type="html">&lt;p&gt;This article shows you how to connect securely to applications and data sources using &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt;. The example connects to an external &lt;a href="https://www.postgresql.org"&gt;PostgreSQL&lt;/a&gt; database in secure Single Sockets Layer (SSL) mode, first locally and then on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. As you will see, it is usually much easier to carry out the integration first on a standalone instance of Red Hat's SSO, and then deploy it on OpenShift. At a high level, we will do the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Step 1: Configure the PostgreSQL server in SSL mode.&lt;/li&gt; &lt;li&gt;Step 2: Configure SSO to connect to the PostgreSQL server in SSL mode.&lt;/li&gt; &lt;li&gt;Step 3: Deploy SSO on OpenShift and connect to the PostgreSQL database using SSL.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Step 1: Configure the PostgreSQL server in SSL mode&lt;/h2&gt; &lt;p&gt;In this section, we install PostgreSQL and change some of its configuration files to enable SSL connections. We also create &lt;a href="https://developers.redhat.com/blog/2021/02/19/x-509-user-certificate-authentication-with-red-hats-single-sign-on-technology"&gt;X.509 certificates&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Install PostgreSQL on Red Hat Enterprise Linux 8&lt;/h3&gt; &lt;p&gt;This section describes how to how to install a PostgreSQL server on &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8. You can find more details on Red Hat's &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/using-databases#using-postgresql"&gt;Using PostgreSQL page&lt;/a&gt;. These commands have to be entered as superuser (root).&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum module install postgresql:13/server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;postgres&lt;/code&gt; superuser is created automatically. Next, initialize the database cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# postgresql-setup --initdb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Red Hat recommends storing database data in the default &lt;code&gt;/var/lib/pgsql/data&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;Next, start the &lt;code&gt;postgresql&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl start postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enable the &lt;code&gt;postgresql&lt;/code&gt; service to start at boot:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl enable postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure the postgres user password&lt;/h3&gt; &lt;p&gt;Choose a password for the &lt;code&gt;postgres&lt;/code&gt; user as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@node-0 data]# su - postgres Last login: Mon Aug 9 05:21:55 EDT 2021 on pts/0 [postgres@node-0 ~]$ psql psql (13.3) Type "help" for help. postgres=# \password postgres Enter new password: Enter it again: postgres=# &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure authentication&lt;/h3&gt; &lt;p&gt;The &lt;code&gt; /var/lib/pgsql/data/pg_hba.conf&lt;/code&gt; file configures client authentication for PostgreSQL databases. Upgrade this file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# TYPE DATABASE USER ADDRESS METHOD local all all md5 host all all 0.0.0.0/0 md5 hostssl all all 0.0.0.0/0 scram-sha-256 &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure the connection&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;/var/lib/pgsql/data/postgresql.conf&lt;/code&gt; file sets database cluster parameters. Upgrade this file to include the following lines:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;listen_addresses = '*' # what IP address(es) to listen on; ... ... # - SSL - ssl = on ssl_ca_file = '/var/lib/pgsql/data/root.crt' ssl_cert_file = '/var/lib/pgsql/data/server.crt #ssl_crl_file = '' ssl_key_file = '/var/lib/pgsql/data/server.key'&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create X.509 certificates&lt;/h3&gt; &lt;p&gt;Log in as the &lt;code&gt;postgres&lt;/code&gt; user and create certificates in the &lt;code&gt;/var/lib/pgsql/data&lt;/code&gt; directory as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cd /var/lib/pgsql/data # log as poostgres user # su - postgres $ openssl genrsa -out server.key 2048 $ openssl rsa -in server.key -out server.key $ chmod 400 server.key $ openssl req -new -key server.key -days 3650 -out server.crt -x509 -subj "/CN=example.com" $ openssl req -new -key server.key -days 3650 -out server.crt -x509 -subj "/CN=example.com" $ cp server.crt root.crt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then restart the PostgreSQL server (as root):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl restart postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the connection to the remote PostgreSQL database&lt;/h3&gt; &lt;p&gt;From a remote machine, test that you can connect to your PostgreSQL database:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ psql -h node-0.postgresql.lab.pnq2.cee.redhat.com -U postgres -W Password for user postgres: psql (10.17, server 13.3) WARNING: psql major version 10, server major version 13. Some psql features might not work. Type "help" for help. postgres=#&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt at the end of the example shows that you are logged in to a running instance of PostgreSQL.&lt;/p&gt; &lt;h2&gt;Step 2: Connect Red Hat's SSO to the PostgreSQL server using SSL&lt;/h2&gt; &lt;p&gt;Now that PostgreSQL is running and able to communicate over SSL, you can set up Red Hat's single sign-on technology to connect to the PostgreSQL database using a JBoss script.&lt;/p&gt; &lt;h3&gt;Create a database for Keycloak&lt;/h3&gt; &lt;p&gt;Connect to the PostgreSQL server and create a database for &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt;, the authentication service that Red Hat's SSO is based on. Start with a &lt;code&gt;CREATE DATABASE&lt;/code&gt; command in a format such as the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-sql"&gt;postgres-# CREATE DATABASE &lt;keycloak-database-name&gt;;&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If there is already an existing Keycloak database, you must delete it with a &lt;code class="language-sql"&gt;DROP DATABASE&lt;/code&gt; command before you create the one used for this example.&lt;/p&gt; &lt;p&gt;For our example, we'll create a database with the name &lt;code&gt;keycloak&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-sql"&gt;postgres-# CREATE DATABASE keycloak;&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The name of the database created will appear in the data source's connection URL.&lt;/p&gt; &lt;h3&gt;Download the PostgreSQL driver&lt;/h3&gt; &lt;p&gt;Download the &lt;code&gt;postgresql&lt;/code&gt; driver from the &lt;a href="https://jdbc.postgresql.org/download.html"&gt;PostgreSQL JDBC driver page&lt;/a&gt;. The driver is in a file named &lt;code&gt;postgresql-42.2.23.jar&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Use a JBoss script to connect to the external database&lt;/h3&gt; &lt;p&gt;A JBoss script named &lt;code&gt;sso-extensions.cli&lt;/code&gt; follows. It contains commands to make Keycloak use PostgreSQL instead of the H2 database that Keycloak uses by default. You need to run the script to allow Keycloak to connect to PostgreSQL in SSL mode.&lt;/p&gt; &lt;p&gt;Before you run the following script, replace the string &lt;code&gt;&lt;postgresql-server-hostname&gt;&lt;/code&gt; with the fully qualified domain name of the PostgreSQL server.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;batch set DB_USERNAME=postgres set DB_PASSWORD=postgres set DRIVER_NAME=postgres set DRIVER_MODULE_NAME=org.postgres set XA_DATABASESOURCE_CLASS="org.postgresql.xa.PGXADataSource" set CONNECTION_URL="jdbc:postgresql://&lt;postgresql-server-hostname&gt;:5432/keycloak?ssl=true;sslfactory=org.postgresql.ssl.NonValidatingFactory" set FILE=/tmp/postgresql-42.2.23.jar module add --name=$DRIVER_MODULE_NAME --resources=$FILE --dependencies=javax.api,javax.resource.api /subsystem=datasources/jdbc-driver=$DRIVER_NAME:add( \ driver-name=$DRIVER_NAME, \ driver-module-name=$DRIVER_MODULE_NAME, \ xa-datasource-class=$XA_DATABASESOURCE_CLASS \ ) /subsystem=datasources/data-source=KeycloakDS:remove() /subsystem=datasources/data-source=KeycloakDS:add( \ jndi-name=java:jboss/datasources/KeycloakDS, \ enabled=true, \ use-java-context=true, \ connection-url=$CONNECTION_URL, \ driver-name=$DRIVER_NAME, \ user-name=$DB_USERNAME, \ password=$DB_PASSWORD \ ) run-batch &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the PostgreSQL database connection&lt;/h3&gt; &lt;p&gt;Now, check whether the previous steps let you connect safely to the PostgreSQL database in SSL mode:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ unzip rh-sso-7.4.0.zip $ cd &lt;rhsso-install-dir&gt; $ bin/sh standalone.sh # Run the PostgreSQL CLI script $ bin/jboss-cli.sh --connect --file=sso-extensions.cli&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, visit the URL &lt;code&gt;https://localhost:8443/auth&lt;/code&gt; to see that the connection is successful.&lt;/p&gt; &lt;h2&gt;Step 3: Deploy SSO on OpenShift and connect to the PostgreSQL database using SSL&lt;/h2&gt; &lt;p&gt;Now we'll move to OpenShift. This section deploys Red Hat's SSO on an OpenShift cluster and connects from there to the external PostgreSQL database in SSL mode. The steps are:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Build a new SSO Docker image using the JBoss command file shown previously.&lt;/li&gt; &lt;li&gt;Deploy Red Hat's SSO on OpenShift using the standard &lt;code&gt;sso74-x509-https&lt;/code&gt; template.&lt;/li&gt; &lt;li&gt;Update the SSO deployment configuration to use the new SSO image.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Build an SSO Docker image&lt;/h3&gt; &lt;p&gt;We'll build a new SSO Docker image to allow connections to the external PostgreSQL driver using SSL. This process is described in detail in the Red Hat documentation for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.4/html/red_hat_single_sign-on_for_openshift_on_openjdk/advanced_concepts#sso-using-custom-jdbc-driver"&gt;using a custom JDBC driver&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Create a new directory and install the following files there:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;postgresql-42.2.23.jar&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;sso-extensions.cli&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Update the &lt;code&gt;sso-extensions.cli&lt;/code&gt; JBoss script to install the driver from the location in &lt;code&gt;/opt/eap/extensions/jdbc-driver.jar:&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt;batch set DB_USERNAME=postgres set DB_PASSWORD=postgres set DRIVER_NAME=postgres set DRIVER_MODULE_NAME=org.postgres set XA_DATABASESOURCE_CLASS="org.postgresql.xa.PGXADataSource" set CONNECTION_URL="jdbc:postgresql://&lt;postgresl-server-hostname&gt;:5432/keycloak?ssl=true;sslfactory=org.postgresql.ssl.NonValidatingFactory" set FILE=/opt/eap/extensions/jdbc-driver.jar module add --name=$DRIVER_MODULE_NAME --resources=$FILE --dependencies=javax.api,javax.resource.api /subsystem=datasources/jdbc-driver=$DRIVER_NAME:add( \ driver-name=$DRIVER_NAME, \ driver-module-name=$DRIVER_MODULE_NAME, \ xa-datasource-class=$XA_DATABASESOURCE_CLASS \ ) /subsystem=datasources/data-source=KeycloakDS:remove() /subsystem=datasources/data-source=KeycloakDS:add( \ jndi-name=java:jboss/datasources/KeycloakDS, \ enabled=true, \ use-java-context=true, \ connection-url=$CONNECTION_URL, \ driver-name=$DRIVER_NAME, \ user-name=$DB_USERNAME, \ password=$DB_PASSWORD \ ) run-batch &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dockerfile contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM rh-sso-7/sso74-openshift-rhel8:latest COPY sso-extensions.cli /opt/eap/extensions/ COPY postgresql-42.2.23.jar /opt/eap/extensions/jdbc-driver.jar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, build a new SSO image using Podman:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ podman build -t localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 . STEP 1: FROM rh-sso-7/sso74-openshift-rhel8:latest STEP 2: COPY sso-extensions.cli /opt/eap/extensions/ --&gt; 9f79713bfc3 STEP 3: COPY postgresql-42.2.23.jar /opt/eap/extensions/jdbc-driver.jar STEP 4: COMMIT localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 --&gt; af34362aeab af34362aeabbdaeb4c3319e42ff8f20c7e3a9cbf6031b6f60301a7ba83d4e558&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Push the new SSO image to quay.io:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;podman login quay.io -u &lt;username&gt; -p &lt;password&gt; $ podman push localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 quay.io/&lt;username&gt;/sso74-external-db-postgres-ssl Getting image source signatures Copying blob fa592e808c80 done Copying blob 329b07dcfb80 done Copying blob 69fa687f24b7 skipped: already exists Copying blob 870b2c4dba9d skipped: already exists Copying blob 1e3f73167579 skipped: already exists Copying config af34362aea done Writing manifest to image destination Copying config af34362aea [--------------------------------------] 0.0b / 4.4KiB Writing manifest to image destination Storing signatures &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure Red Hat's SSO on OpenShift&lt;/h3&gt; &lt;p&gt;Create a new project in OpenShift and deploy the &lt;code&gt;sso74-x509-https&lt;/code&gt; template there. This template initially connects to the default H2 database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project sso-74-external-db-ssl Now using project "sso-74-external-db-ssl" on server "https://openshift.example.com:443" $ oc process sso74-x509-https SSO_ADMIN_USERNAME=admin SSO_ADMIN_PASSWORD=password -n openshift -o yaml &gt; sso74-x509-https.yaml $ oc create -f sso74-x509-https.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now check the status of the SSO server:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get pods NAME READY STATUS RESTARTS AGE sso-1-x625p 1/1 Running 0 42s $ oc status In project sso-74-external-db-ssl on server https://openshift.example.com:443 svc/sso-ping (headless):8888 https://sso-sso-74-external-db-ssl.apps.example.com (reencrypt) (svc/sso) dc/sso deploys openshift/sso74-openshift-rhel8:7.4 deployment #1 deployed about a minute ago - 1 pod View details with 'oc describe &lt;resource&gt;/&lt;name&gt;' or list everything with 'oc get all'. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also connect to the SSO admin console at the URL &lt;code&gt;https://sso-sso-74-external-db-ssl.apps.example.com/auth&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Update the deployment with the new SSO image&lt;/h3&gt; &lt;p&gt;Update the deployment configuration as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc edit dc/sso&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace the SSO image with:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; ... ... image: quay.io/orivat/sso74-external-db-postgres-ssl:latest imagePullPolicy: Always livenessProbe: .... .... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also update the &lt;code&gt;triggers&lt;/code&gt; section so that it pulls new SSO images automatically from quay.io instead of from Red Hat's SSO registry:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;.... .... triggers: - imageChangeParams: automatic: true containerNames: - sso from: kind: ImageStreamTag name: sso74-openshift-rhel8:7.4 namespace: openshift type: ImageChange - type: ConfigChange .... ....&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the SSO server status&lt;/h3&gt; &lt;p&gt;You can see the status of the SSO server as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; oc status In project sso-74-external-db-ssl on server https://openshift.example.com:443 svc/sso-ping (headless):8888 https://sso-sso-74-external-db-ssl.apps.example.com (reencrypt) (svc/sso) dc/sso deploys quay.io/&lt;username&gt;/sso74-external-db-postgres-ssl:latest deployment #2 failed 34 minutes ago: config change deployment #1 deployed about an hour ago - 1 pod &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output indicates that it has pulled the image from quay.io.&lt;/p&gt; &lt;p&gt;It is now possible to connect safely from the SSO admin console to the external PostgreSQL database in SSL mode at the following URL:&lt;/p&gt; &lt;pre&gt; https://sso-sso-74-external-db-ssl.apps.example.com/auth&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed you how to use Red Hat's single sign-on technology to connect from OpenShift to an external PostgreSQL database over SSL. We used a new custom SSO Docker image, which contains the PostgreSQL driver and a JBoss configuration script to connect to the external database in SSL mode. You can generalize the approach we've followed here to any Openshift project using Red Hat's SSO.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/18/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology" title="Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology"&gt;Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WDsW_vxS2Eg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Olivier Rivat</dc:creator><dc:date>2021-08-18T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/18/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology</feedburner:origLink></entry><entry><title type="html">LRA annotation checker Maven plugin</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4YkGENDajkI/lra-annotation-checker-maven-plugin.html" /><author><name>Ondra Chaloupka</name></author><id>https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html</id><updated>2021-08-17T10:56:00Z</updated><content type="html">With the release of the LRA (Long Running Actions) specification in Narayana team works on integrating the to various application runtimes. Currently it's and . (Camel is a third platform but it does in a way not depending on LRA annotations defined in the specification.). NOTE:If you want to get introduction what is LRA and what is good for you can read some of the already published articles (, ). At this time when the LRA  can be easily grab and used within the application runtimes it may come some difficulty on precise use of the . The defines different "requirements" the LRA application has to follow to work correctly. Some are basic as "the LRA annotated class must contain at least one of the methods annotated with @Compensate or @AfterLRA" or that the LRA annotated JAX-RS endpoints has predefined HTTP methods to be declared with. For example the /requires the @PUT method while the requires the @DELETE and requires the @GET. When the specific LRA contract rule is violated the developer will find them at the deployment time with the being thrown. But time of the deployment could be a bit late to find just a forgotten annotation required by the LRA specification. With that idea in mind Narayana offers a Maven plugin project . The developer working with the LRA introduces the dependency to the LRA annotation with artifact . He codes the application and then he can introduce the Maven plugin of the LRA checker to be run during Maven phase process-classes. The developer needs to point to the plugin goal check to get the verification being run. The snippets that can be placed to the project pom.xml is following. The plugin Maven artifact coordinates is io.narayana:maven-plugin-lra-annotations_1.0:1.0.0.Beta1 ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;io.narayana&lt;/groupId&gt; &lt;artifactId&gt;maven-plugin-lra-annotations_1.0&lt;/artifactId&gt; &lt;version&gt;1.0.0.Beta1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ... When plugin is loaded it searches for classes available at path ${project.build.directory}/classes(i.e., target/classes) and tries to find if the application preserve the rules defined by the LRA specification. When not then the Maven build fails reporting what error happens. Such an error is in format of [error id]:[description]. Example of is [ERROR] Failed to execute goal io.narayana:maven-plugin-lra-annotations_1.0:1.0.0.Beta1:check (default) on project lra-annotation-checker-maven-plugin-test: LRA annotation errors: [ERROR] [[4]]-&gt;   1: The class annotated with org.eclipse.microprofile.lra.annotation.ws.rs.LRA missing at least one of the annotations Compensate or AfterLRA Class: io.narayana.LRAParticipantResource;   2: Multiple annotations of the same type are used. Only one per the class is expected. Multiple annotations 'org.eclipse.microprofile.lra.annotation.Status' in the class 'class io.narayana.LRAParticipantResource' on methods [status, status2].;   4: Wrong method signature for non JAX-RS resource method. Signature for annotation 'org.eclipse.microprofile.lra.annotation.Forget' in the class 'io.narayana.LRAParticipantResource' on method 'forget'. It should be 'public void/CompletionStage/ParticipantStatus forget(java.net.URI lraId, java.net.URI parentId)';   5: Wrong complementary annotation of JAX-RS resource method. Method 'complete' of class 'class io.narayana.LRAParticipantResource' annotated with 'org.eclipse.microprofile.lra.annotation.Complete' misses complementary annotation javax.ws.rs.PUT. The plugin can be configured with two parameters (placed under &lt;configuration&gt;under the &lt;plugin&gt; element). Attribute Description Default paths Paths searched for classes to be checked. Point to a directory or jar file. Multiple paths are delimited with a comma. ${project.build.directory}/classes failWhenPathNotExist When some path defined within argument paths does not exist then Maven build may fail or resume with information that the path is not available. true All the points described in this article can be seen and tested in an example at . The plugin configuration can be seen in the of the same project. Any comments, ideas for enhancement and bug reports are welcomed. The project LRA annotation checker Maven plugin is placed in the . The issues can be submitted via JBTM issue tracker at . Hopefully this small plugin provides a better experience for any developer working with the LRA and LRA Narayana implementation in particular.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4YkGENDajkI" height="1" width="1" alt=""/&gt;</content><dc:creator>Ondra Chaloupka</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html</feedburner:origLink></entry><entry><title>Explore new features in SystemTap 4.5.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3l_i9rAxnAU/explore-new-features-systemtap-450" /><author><name>Stan Cox</name></author><id>de3aa528-f93b-4663-b244-41271b5a78bb</id><updated>2021-08-16T07:00:00Z</updated><published>2021-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/systemtap_beginners_guide/understanding-how-systemtap-works"&gt;SystemTap&lt;/a&gt; uses a command-line interface (CLI) and a scripting language to write instrumentation for a live, running kernel or a user-space application. A SystemTap script associates handlers with named events. When a specified event occurs, the default SystemTap kernel runtime runs the handler in the kernel like a quick subroutine and then resumes.&lt;/p&gt; &lt;p&gt;This article lays out the new features in SystemTap 4.5.0. This version will appear in &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9.0. The features fall into three general categories: context variable access, aliasing, and the &lt;a href="https://www.kernel.org/doc/html/latest/bpf/index.html"&gt;Berkeley Packet Filter (BPF)&lt;/a&gt; back-end.&lt;/p&gt; &lt;h2&gt;Context variable improvements&lt;/h2&gt; &lt;p&gt;These features deal with enumerator access, thread-local storage access, floating-point variable access, and context variable access within functions.&lt;/p&gt; &lt;h3&gt;Enumerator access&lt;/h3&gt; &lt;p&gt;Enumerator values can now be accessed as &lt;code&gt;$context&lt;/code&gt; variables, specified through a dollar sign. For example, consider the following C program source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;typedef enum { at='@',sharp='#' } symbols; symbols symbol = at;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Suppose you run the following probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("symbol=%c\n",$symbol)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;symbol=@&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Only unscoped enumerators are currently supported; scoped enumerators such as the following are not:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;enum class Color { red, green = 20, blue };&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Implicit thread-local storage&lt;/h3&gt; &lt;p&gt;Implicit thread-local storage consists of variables that have a different instance in each thread and persist as long as the thread is alive. The following C declaration defines an implicit thread-local variable named &lt;code&gt;tls&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt; __thread unsigned long tls = 99;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each thread has its own instance of &lt;code&gt;tls&lt;/code&gt; and can change it without affecting the variable of the same name in other threads. On GNU/Linux systems, you can obtain more information about thread-local variables through the command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;info gcc 'C Extensions' Thread-Local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If a probe handler is defined for the same module that defines &lt;code&gt;tls&lt;/code&gt;, the handler can access the value via &lt;code&gt;@var("tls")&lt;/code&gt;. If &lt;code&gt;tls&lt;/code&gt; is defined in another module, e.g., the shared object &lt;code&gt;libtls.so&lt;/code&gt;, the value of &lt;code&gt;tls&lt;/code&gt; can be accessed via &lt;code&gt;@var("tls","libtls.so")&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;One common use of this feature is to read the &lt;code&gt;errno&lt;/code&gt; value, which is set by many library functions. Take for example the following C code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;infile = fopen (file_does_not_exist, "r");&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When a statement like this one tries to open a file that doesn't exist, the system returns an &lt;code&gt;errno&lt;/code&gt; value of &lt;code&gt;ENOENT&lt;/code&gt;. After issuing the &lt;code&gt;fopen&lt;/code&gt; call, assume you run the following probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("errno=%d %s\n",@errno,errno_str(@errno))}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;errno=2 ENOENT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Support for tracking virtual memory addresses was added to the stapdyn back-end. That feature enables the SystemTap Dyninst back-end to access global and static variables, including thread-local variables.&lt;/p&gt; &lt;p&gt;Implicit thread-local variables can be accessed as &lt;code&gt;$context&lt;/code&gt; variables on the x86_64, PowerPC, and s390 architectures.&lt;/p&gt; &lt;h3&gt;Floating-point variables&lt;/h3&gt; &lt;p&gt;A SystemTap probe handler can now access floating-point variables as &lt;code&gt;$context&lt;/code&gt; variables. Start with the following C declaration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;float pi = 3.14159&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("%s\n",fp_to_string($pi,5))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;3.14159&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;%e&lt;/code&gt; and &lt;code&gt;%f&lt;/code&gt; conversion specifiers of &lt;code&gt;printf&lt;/code&gt; are not currently supported by the SystemTap &lt;code&gt;printf&lt;/code&gt; statement. The &lt;code&gt;fp_to_string&lt;/code&gt; function can be used instead. Other useful floating-point functions include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;string_to_fp (string)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;long_to_fp (long)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_to_long (float)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_add (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_sub (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_mul (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_div (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_sqrt (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_eq (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_le (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_lt (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;32-bit floats are automatically widened to doubles.&lt;/p&gt; &lt;h3&gt;Access context variables inside functions&lt;/h3&gt; &lt;p&gt;Functions may now refer to &lt;code&gt;$context&lt;/code&gt; variables and to operators that reference &lt;code&gt;$context&lt;/code&gt; variables such as &lt;code&gt;$$vars&lt;/code&gt; and &lt;code&gt;$$locals&lt;/code&gt;. Previously these references could appear only inside probe handlers.&lt;/p&gt; &lt;p&gt;Consider the following C code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;int handle_object1 () { int attr1; int attr2; attr1 = 9; attr2 = 99; types type = thing1; return 0; } int handle_object2 () { int attr1; int attr2; attr1 = 8; attr2 = 88; types type = thing2; return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Probe handlers can refer to a common &lt;code&gt;binary_op_vals&lt;/code&gt; function that accesses the context variables &lt;code&gt;attr1&lt;/code&gt; and &lt;code&gt;attr2&lt;/code&gt;, which are common to both the &lt;code&gt;handle_object1&lt;/code&gt; and &lt;code&gt;handle_object2&lt;/code&gt; functions. The &lt;code&gt;binary_op_vals&lt;/code&gt; function can also refer to the &lt;code&gt;$context&lt;/code&gt; variable operators $$&lt;code&gt;vars&lt;/code&gt; and $$&lt;code&gt;locals&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;function binary_op_vals () { printf ("type = %d attribute 1 = %d attribute 2 = %d\n", $type, $attr1, $attr2); } probe process.statement("handle_object1@*:11") { binary_op_vals (); } probe process.statement("handle_object2@*:22") { binary_op_vals (); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding probe handlers display:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;type = 0 attribute 1 = 9 attribute 2 = 99 type = 1 attribute 1 = 8 attribute 2 = 88&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;BPF back-end improvements&lt;/h2&gt; &lt;p&gt;Values in user space can now be accessed with functions such as &lt;code&gt;user_string&lt;/code&gt;, &lt;code&gt;user_int&lt;/code&gt;, and &lt;code&gt;user_long&lt;/code&gt;. Consider the following C source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;long ipf = 12; long *ipfp = &amp;ipf;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the following &lt;code&gt;stap&lt;/code&gt; command from the SystemTap CLI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;stap --bpf --disable-cache -e 'probe process.statement("main@*:22") {printf("%d\n",user_long($ipfp))}' -c ./tstustr&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;12&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note the use of the &lt;code&gt;-c&lt;/code&gt; option to specify the target program. The &lt;code&gt;systemtap -c &lt;command&gt;&lt;/code&gt; option can now be used with the BPF back-end. This option sets the SystemTap target process to the process ID (PID) of the running command.&lt;/p&gt; &lt;h2&gt;Aliasing improvements&lt;/h2&gt; &lt;p&gt;An alias provides a mechanism for augmenting the handlers that are taken by another probe. The alias handlers can be invoked either before the handler, known as a &lt;em&gt;prologue&lt;/em&gt;, or after the handler, known as an &lt;em&gt;epilogue&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;It is now possible to define an alias with both a prologue and an epilogue. For example, a probe follows that has a prologue and an epilogue. The probe sets the global variable &lt;code&gt;file_descriptors&lt;/code&gt; whenever the &lt;code&gt;open&lt;/code&gt; or &lt;code&gt;openat&lt;/code&gt; syscalls are invoked:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;private global filenames private global file_descriptors probe file_open = syscall.{open,openat} { delete filenames[tid()] },{ if (! (tid() in filenames)) filenames[tid()] = filename; } probe file_open_return = syscall.{open.return,openat.return} { if (tid() in filenames) file_descriptors[tid(),retval] = filenames[tid()] } // These alias definitions can be used to only record file descriptors // that match a given pathname. probe file_open { %($# &gt; 0 %? if (strpos(filename,@1) == -1) next %) } probe file_open_return { } probe end { foreach ([t,f] in file_descriptors) { printf ("%d %d %s\n", t, f, file_descriptors[t,f]) } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If this script is named &lt;code&gt;alias.stp&lt;/code&gt;, you can execute it through:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;stap alias.stp /home/user&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script displays output such as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2309651 101 "/home/user/path1" 2310349 459 "/home/user/path2"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Additionally, the &lt;code&gt;@probewrite&lt;/code&gt; predicate can be used to determine whether a variable has already been written to. For example, the prologue just shown could be defined as follows, to check that &lt;code&gt;filesnames&lt;/code&gt; has already been written to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt; if (@probewrite(filenames)) delete filenames[tid()]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;SystemTap in Red Hat Enterprise Linux 9 fills several gaps left in previous versions and provides many conveniences to developers investigating kernel and application behavior. These new features might inspire you to try moving to a lower level of the system through SystemTap.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/16/explore-new-features-systemtap-450" title="Explore new features in SystemTap 4.5.0"&gt;Explore new features in SystemTap 4.5.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3l_i9rAxnAU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Stan Cox</dc:creator><dc:date>2021-08-16T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/16/explore-new-features-systemtap-450</feedburner:origLink></entry><entry><title>Test container images in Red Hat OpenShift 4 with Ansible and CI/CD</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bljgoDky4Wg/test-container-images-red-hat-openshift-4-ansible-and-cicd" /><author><name>Petr Hracek</name></author><id>006c2cb0-68fe-4e13-8f71-d003e718625e</id><updated>2021-08-13T07:00:00Z</updated><published>2021-08-13T07:00:00Z</published><summary type="html">&lt;p&gt;Several repositories offer ready-made &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images for &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; and other systems running &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;. The InterOp team at Red Hat tests these application images in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. To simplify the integration of tests into the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous delivery (CI/CD)&lt;/a&gt; process, we are adding &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; playbooks to the repositories that host the container images. The Red Hat Software Collections &lt;a href="https://github.com/sclorg/"&gt;GitHub repository&lt;/a&gt; currently has the first of these Ansible playbooks, but we will add playbooks to other repositories over time.&lt;/p&gt; &lt;p&gt;This article shows how to submit a test to a repository, and how to download the tests if you want to run them in your own container environment.&lt;/p&gt; &lt;h2&gt;Parameters for testing a container&lt;/h2&gt; &lt;p&gt;In order to test a container under a Red Hat OpenShift 4 environment, the developer has to provide information about where to download, deploy, and test the container. The necessary information is illustrated in the following playbook for a PostgreSQL container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;registry_redhat_io: "rhscl/postgresql-10-rhel7" tag_name: "postgresql:10-el7" deployment: "oc new-app postgresql:10-el7~https://github.com/sclorg/postgresql-container.git \ --name new-postgresql \ --context-dir examples/extending-image/ \ -e POSTGRESQL_USER=user \ -e POSTGRESQL_DATABASE=db \ -e POSTGRESQL_PASSWORD=password" pod_name: "new-postgresql" add_route: true test_exec_command: "./files/check_postgresql_container.sh" expected_exec_result: "FINE" check_curl_output: “SOMETHING from curl output” scl_url: "postgresql-container" is_name: "postgresql"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The meanings of the fields follow:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;registry_redhat_io&lt;/code&gt;: The image in the registry.redhat.io catalog, including the namespace, which is &lt;code&gt;rhscl&lt;/code&gt; in this case.&lt;/li&gt; &lt;li&gt;&lt;code&gt;tag_name&lt;/code&gt;: The tag name of the image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;deployment&lt;/code&gt;: The command that deploys the image into the OpenShift environment.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pod_name&lt;/code&gt;: name of the pod in the OpenShift namespace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;add_route&lt;/code&gt;: Whether the route should be exposed, where the default is not to expose it.&lt;/li&gt; &lt;li&gt;&lt;code&gt;test_exec_command&lt;/code&gt;: The file that performs the test.&lt;/li&gt; &lt;li&gt;&lt;code&gt;expected_exec_result&lt;/code&gt;: A string expected from executing the &lt;code&gt;test_exec_command&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;check_curl_output&lt;/code&gt;: A substring from the expected output of a &lt;code&gt;curl&lt;/code&gt; command.&lt;/li&gt; &lt;li&gt;&lt;code&gt;scl_url&lt;/code&gt;: The repository name, for the Software Collections repositories only.&lt;/li&gt; &lt;li&gt;&lt;code&gt;is_name&lt;/code&gt;: The imagestream of the container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To submit a test for public use, file a pull request at the site hosting the tests. There is currently one site at the &lt;a href="https://github.com/sclorg/ansible-tests/pulls"&gt;Software Collections GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Storing a test locally&lt;/h2&gt; &lt;p&gt;If you want to keep the test in your private environment instead of sharing the test, you can download our test suite and add your test to it as follows.&lt;/p&gt; &lt;p&gt;Clone the test repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/sclorg/ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add your container test suite to the &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/deploy-and-test.yml"&gt;main Ansible playbook&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Running a test&lt;/h2&gt; &lt;p&gt;This section assumes that you are running an OpenShift 4 cluster.&lt;/p&gt; &lt;h3&gt;Downloading the OpenShift 4 client&lt;/h3&gt; &lt;p&gt;The latest version of the OpenShift 4 client, 4.6.18, can be obtained for your system at &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.618/"&gt;this mirror site&lt;/a&gt;. Download the ZIP file and unpack it through:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tar -xzvf &lt;FILE&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The kubeconfig file&lt;/h3&gt; &lt;p&gt;Tests refer to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"&gt;kubeconfig&lt;/a&gt; file, so you need to point the &lt;code&gt;KUBECONFIG&lt;/code&gt; environment variable to the file. Ask your OpenShift 4 cluster administrator for the location of the file, then insert the path into the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KUBECONFIG=&lt;path_to_kubeconfig&gt;/kubeconfig&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Running your test&lt;/h3&gt; &lt;p&gt;Switch to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the test as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ make ocp4-tests EXT_TEST=&lt;your_test_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details are available in a &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/README_ocp4.md"&gt;README file&lt;/a&gt; in the repository.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article showed a new method that makes it easy to add a test for your container to a test suite and run the test in an OpenShift 4 cluster that has been set up by an administrator. You can keep the test in your own environment, but we recommend that you heed the Red Hat phrase, "It’s better to share."&lt;/p&gt; &lt;p&gt;By providing your test to the InterOp team, you can get the container tested during each feature freeze or code freeze done by the OpenShift 4 development team. Feel free to &lt;a href="https://issues.redhat.com/projects/LPINTEROP/issues/LPINTEROP-1858?filter=allopenissues"&gt;contact the InterOp team at Red Hat&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd" title="Test container images in Red Hat OpenShift 4 with Ansible and CI/CD"&gt;Test container images in Red Hat OpenShift 4 with Ansible and CI/CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bljgoDky4Wg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Petr Hracek</dc:creator><dc:date>2021-08-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd</feedburner:origLink></entry><entry><title type="html">How to Capture Business Decisions using DMN: Introduction to Some Basic Patterns and Their Value</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/DZN0Qbu7ROo/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2021/08/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html</id><updated>2021-08-12T08:50:00Z</updated><content type="html">I am very glad for the opportunity to have with Denis Gagné CEO &amp;amp; CTO of Trisotech! 1. how business analysts can use the DMN open-standard to capture the requirements for operational business decisions 2. some of the recurring basic patterns in modeling (Q&amp;amp;A, Scoring, Classification and Categorisation, Ranking..) 3. how to transform these decision models into actual In the following you can find the recording, as well as a brief summary of some key highlights for the patterns. WEBINAR RECORDING IIBA Webinar recording Businesses continuously make Business Decisions. Some of these decisions are strategic business decisions, but a lot are operational business decisions taken every day within every transaction. With the ever-increasing number of laws and regulations that may apply or regulate these operational business decisions, business analysts are more often called upon to document/specify how these business decisions are to be taken in order to provide transparency and to offer auditable traces of the actual decisions taken. In this insightful session, we will introduce how business analysts can use DMN to capture the requirements for operational business decisions, some of the recurring basic patterns in modeling these business decisions and will even show how to transform these decision models into actual executable business decision services. PATTERN: Q&amp;amp;A Not to be confused with the generally applicable multiple-choice paradigm, on of the key aspects of DMN models is that each Decision is meant to answer a question, usually a business related question or a domain question. This Q&amp;amp;A pattern is a built-in of the DMN standard, thanks to the optional attributes "question" and "allowedAnswers", which business analyst and subject matter expert modelers often use to describe using natural language to complement the modeled decision logic semantic. PATTERN: SCORING Input variables, described as InputData, are often analysed and weighted into a score, typically to be later used in the DMN model with other patterns (such as categorisation) or directly with decision logic such as thresholds. Examples: PATTERN: CLASSIFICATION In this pattern, model variables are recognized, differentiated and classified to be better understood; usually this is done to verbalise classes and often using a Score as an input (ref above). It is important to be aware of several types of classification: * Classification: separating based on class labels * Clustering: separating based similarities without class labels * Categorization: subsuming classes (to realize a taxonomy) * Segmentation: complete and disjunct categorization Examples: PATTERN: RANKING In this pattern, the position, or rank, of each item in a collection is determined. This pattern is usually a bit more complex to implement if compared to the previous one, requiring iteration with sorting. Fortunately the DMN standard gives all the tools to implement this pattern more easily! For example: TRANSFORM DMN MODELS INTO EXECUTABLE BUSINESS DECISION SERVICES Have you found this content interesting? Don’t forget a great advantage of DMN models is that they can be immediately deployed as executable services, thanks to the and ! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/DZN0Qbu7ROo" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html</feedburner:origLink></entry><entry><title>Build and deploy microservices with Kubernetes and Dapr</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/sRPE9yNLZBM/build-and-deploy-microservices-kubernetes-and-dapr" /><author><name>Ip Sam</name></author><id>e3e788c4-5230-41c1-9cb2-0168445a6352</id><updated>2021-08-12T07:00:00Z</updated><published>2021-08-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://dapr.io/"&gt;Dapr&lt;/a&gt; (Distributed Application Runtime) provides an event-driven, portable runtime for building distributed &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;. The project is useful for both stateless or stateful applications on the cloud and at the network edge. A new &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project from Microsoft, Dapr embraces a diversity of languages and development frameworks. The project is a natural fit for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to install Dapr and walks you through the process of building a sample application on Kubernetes.&lt;/p&gt; &lt;h2&gt;Dapr components&lt;/h2&gt; &lt;p&gt;The basic architecture of Dapr consists of &lt;em&gt;building blocks&lt;/em&gt;, which in turn contain &lt;em&gt;components&lt;/em&gt; (Figure 1). The building blocks offer distributed system capabilities such as publications and subscriptions, state management, resource bindings, and distributed tracing. Each building block exposes a public &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API&lt;/a&gt; that is called from your code. Table 1 describes the different types of Dapr building blocks.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_bb.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_bb.png?itok=8bNmOMNu" width="522" height="591" alt="Dapr offers APIs through building block, each containing multiple components to implement the APIs." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Components in a Dapr building block. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1. Dapr building blocks.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Component &lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Function&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Service-to-service invocation&lt;/td&gt; &lt;td&gt;Perform direct, secure, service-to-service method calls.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;State management&lt;/td&gt; &lt;td&gt;Create long-running stateless and stateful services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Publish and subscribe&lt;/td&gt; &lt;td&gt;Provide secure and scalable messaging between services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Resource bindings and triggers&lt;/td&gt; &lt;td&gt;Trigger code through events from a large array of input. Output binding to external resources including databases and queues.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Actors&lt;/td&gt; &lt;td&gt;Encapsulate code and data in reusable actor objects as a common micro-service design pattern.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Observability&lt;/td&gt; &lt;td&gt;See and measure the message calls across components and network services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Secrets&lt;/td&gt; &lt;td&gt;Securely access secrets from your applications.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Components encapsulate the implementations for a building block's API. Components include Ceph, PostgreSQL, MySQL, Redis, and MongoDB. Many of the components are pluggable, so that one implementation can be swapped out for another. Each component has an interface definition.&lt;/p&gt; &lt;p&gt;The building blocks integrate with application code and different cloud service providers, as shown in Figure 2. The architecture for Dapr on Red Hat OpenShift and Kubernetes is shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_integration.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_integration.png?itok=dbqc0ipl" width="600" height="289" alt="Dapr can work with multiple platforms." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Dapr architecture with cloud providers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_kubernetes.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_kubernetes.png?itok=5hwfVCdj" width="600" height="287" alt="Dapr integrates as pods with Kubernetes and OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Dapr components with Kubernetes and Red Hat OpenShift.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How to install Dapr&lt;/h2&gt; &lt;p&gt;Follow these steps in this section to install Dapr in your OpenShift cluster.&lt;/p&gt; &lt;p&gt;First, download Dapr from GitHub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Log in to your OpenShift cluster as an administrator. Check your login status:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc whoami&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are using a Helm repository to install Dapr, you need to add the Dapr URL to the Helm repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo add dapr https://daprio.azurecr.io/helm/v1/repo "dapr" has been added to your repositories&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you can update the Helm repository to get the latest changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ash"&gt;$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the "dapr" chart repository Update Complete. ⎈ Happy Helming!⎈&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new namespace in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create namespace dapr-system namespace/dapr-system created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install Dapr to the new namespace that you just created:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install dapr dapr/dapr --namespace dapr-system NAME: dapr LAST DEPLOYED: Mon Apr 6 12:48:40 2020 NAMESPACE: dapr-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Dapr: High-performance, lightweight serverless runtime for cloud and edge Your release is named dapr. To get started with Dapr, we recommend using our samples page: https://github.com/dapr/samples For more information on running Dapr, visit: https://dapr.io&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following Dapr pods will be created:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dapr-operator:&lt;/code&gt; Manages components and services endpoints for Dapr (state stores, pub-subs, etc.).&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-sidecar-injector&lt;/code&gt;: Injects Dapr into annotated pods.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-placement&lt;/code&gt;: Used for actors only. Creates mapping tables that map actor instances to pods.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-sentry&lt;/code&gt;: Manages transport layer security and acts as a certificate authority.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Check your pods' status to make sure that they are in the running state:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods -n dapr-system -w NAME READY STATUS RESTARTS AGE dapr-operator-7d9668fd8b-skmbh 1/1 Running 0 68s dapr-placement-7dbcc6bf59-zxx4s 1/1 Running 0 68s dapr-sentry-756f7799fd-xm57l 1/1 Running 0 68s dapr-sidecar-injector-7849f77c4b-l789t 1/1 Running 0 68s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Build a sample application in Node.js&lt;/h2&gt; &lt;p&gt;This section shows how to get Dapr running in Red Hat OpenShift and deploy a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application that handles retail orders. Another application, written in &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, generates messages containing the orders. The Node.js application subscribes to messages and persists them, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_endpoints.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_endpoints.png?itok=RcZOHBGe" width="600" height="330" alt="The example application offers GET and POST endpoints, mediated by Dapr, and persistent state stores." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: GET and POST endpoints, mediated by Dapr, and persistent state stores. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 5 shows the Dapr components for the producer and consumer applications.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_apps.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_apps.png?itok=zW_qkAt2" width="600" height="305" alt="The Dapr APIs communicate with the example's two apps." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Dapr and the example's two apps. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The implementation uses Redis as a state store for data persistence.&lt;/p&gt; &lt;p&gt;To access the code, clone the following repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/dapr/samples.git $ cd samples/1.hello-world&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; code for the order POST endpoint follows. The application persists the order information in Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;app.post('/neworder', (req, res) =&gt; { const data = req.body.data; const orderId = data.orderId; console.log("Got a new order! Order ID: " + orderId); const state = [{ key: "order", value: data }]; fetch(stateUrl, { method: "POST", body: JSON.stringify(state), headers: { "Content-Type": "application/json" } }) .then((response) =&gt; { if (!response.ok) { throw "Failed to persist state."; } console.log("Successfully persisted state."); res.status(200).send(); }) .catch((error) =&gt; { console.log(error); res.status(500).send({message: error}); }); }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The code for the order GET endpoint follows. The application retrieves the latest order information from Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;app.get('/order', (_req, res) =&gt; { fetch(`${stateUrl}/order`) .then((response) =&gt; { if (!response.ok) { throw "Could not get state."; } return response.text(); }).then((orders) =&gt; { res.send(orders); }).catch((error) =&gt; { console.log(error); res.status(500).send({message: error}); }); }); &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the application on Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;Install Redis in OpenShift from a Helm chart as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo add bitnami https://charts.bitnami.com/bitnami "bitnami" has been added to your repositories $ helm install redis bitnami/redis NAME: redis LAST DEPLOYED: Mon Apr 6 12:58:12 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Extract the secret from the default namespace for Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get secret --namespace default redis -o jsonpath="{.data.redis-password}" | base64 --decode pbOe0CgPsu&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now update the &lt;code&gt;redis.yaml&lt;/code&gt; file in the deployment directory. Change &lt;code&gt;redisHost&lt;/code&gt; to &lt;code&gt;redis-master:6379&lt;/code&gt; and &lt;code&gt;redisPassword&lt;/code&gt; to the value extracted in the previous step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: statestore spec: type: state.redis metadata: - name: redisHost value: redis-master:6379 - name: redisPassword value: Pnkkpan9gs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the Redis resource in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f deploy/redis.yaml component.dapr.io/statestore created &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create the Node.js and Python applications:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f deploy/node.yaml service/nodeapp created deployment.apps/nodeapp created $ oc apply -f deploy/python.yaml deployment.apps/pythonapp created &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To examine your Dapr pods in OpenShift, you can run the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods -n dapr-system -w NAME READY STATUS RESTARTS AGE dapr-operator-7c6799878d-v4stm 1/1 Running 14 54m dapr-placement-76c99b79bb-n6sfw 1/1 Running 0 54m dapr-sentry-5644b86cf9-8fjpv 1/1 Running 0 54m dapr-sidecar-injector-84c5578f8d-d6dp4 1/1 Running 0 54m nodeapp-548959b4b9-5rdnl 2/2 Running 0 21m pythonapp-79c9b55c8f-p7gng 2/2 Running 0 18m redis-master-0 1/1 Running 0 52m redis-slave-0 1/1 Running 1 52m redis-slave-1 1/1 Running 0 49m &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can test the Dapr application by viewing the logs coming out from the pod and looking at what is consuming the messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs pod/nodeapp-548959b4b9-5rdnl -c node Node App listening on port 3000! Got a new order! Order ID: 1 Successfully persisted state Got a new order! Order ID: 2 Successfully persisted state Got a new order! Order ID: 3 Successfully persisted state&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also expose a route from the &lt;code&gt;nodeapp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc expose svc nodeapp route.route.openshift.io/nodeapp exposed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, invoke the &lt;code&gt;nodeapp&lt;/code&gt; order endpoint to confirm the successful persistence:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl nodeapp-default.apps-crc.testing/order {"orderID":"42"} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You have just successfully deployed a Dapr application on OpenShift. You might want to update the sample code to fit your scenario by forking the &lt;a href="https://github.com/dapr/samples.git"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Dapr with Kubernetes or Red Hat OpenShift enables the easy development of event-driven, stateful microservices. Dapr also provides consistency and portability via standard open APIs. It is an open source project that works well with numerous programming languages and development frameworks.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/12/build-and-deploy-microservices-kubernetes-and-dapr" title="Build and deploy microservices with Kubernetes and Dapr"&gt;Build and deploy microservices with Kubernetes and Dapr&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/sRPE9yNLZBM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ip Sam</dc:creator><dc:date>2021-08-12T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/12/build-and-deploy-microservices-kubernetes-and-dapr</feedburner:origLink></entry></feed>
